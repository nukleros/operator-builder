{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#operator-builder","title":"Operator Builder","text":"<p>Accelerate the development of Kubernetes operators.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>User documentation can be found at operatorbuilder.io.</p> <p>Developer documentation can be found here.</p>"},{"location":"#what-is-a-kubernetes-operator","title":"What is a Kubernetes Operator?","text":"<p>A Kubernetes operator takes human operational knowledge for managing workloads on Kubernetes and encodes it into a piece of software.  Operators programmatically perform the functions that a human engineer would otherwise have to execute in managing software systems on Kubernetes.  In practice, they usually consist of:</p> <ol> <li>One or more custom resource definitions (CRDs) that extend the Kubernetes API    and provide a data model for defining the desired state of some software    system.</li> <li>A custom Kubernetes controller that watches the custom resource instances and    reconciles the desired state defined in them.</li> </ol> <p>You can find more info on the Kubernetes operator pattern in the Kubernetes docs.</p>"},{"location":"#types-of-operators","title":"Types of Operators","text":"<p>There is a vast amount of functionality that can be implemented in a Kubernetes operator.  You are limited by only by the Kubernetes API and the things you can do with Go (or your programming language of choice).  That said, there are general categories we can break Kubernetes operators into.</p> <ul> <li>App Stack Management:  This category of operators provide an abstraction for   deploying and managing particular applications.  They are most helpful with   sophisticated stateful apps that are relatively involved to operate and those   apps that have multiple workload components.  With this kind of operator, a   user often creates, updates and deletes a single custom resource instance that   triggers a Kubernetes controller to create, update and delete all the   Kubernetes resources that constitute that app.  The app often consists of   dozens of different resources so this kind of operator is extremely helpful   in reducing operational toil and improving consistency and reliability.   Popular examples include the Prometheus Operator and   various database operators.  These are a very common category of Kubernetes   operator.   </li> <li>External Integrations:  This kind of operator uses custom resources to define   resources external to Kubernetes such as cloud provider resources.  The AWS   Controllers for Kubernetes   is a good example of this.</li> <li>Workload Support Systems:  Some operators don't directly manage Kubernetes or   external resources, but instead provide configuration support services for   applications.  They often watch resources created by other systems and take   config actions to support different workloads.  A good example of this is   cert-manager which is commonly used to manage TLS   assets based on Ingress resource configurations.</li> </ul>"},{"location":"#when-to-use-operator-builder","title":"When to Use Operator Builder","text":"<p>Operator Builder speeds up the development of the first kind of operator: App Stack Managers.  It is a command line tool that ingests Kubernetes manifests and generates the source code for a working Kubernetes operator based on the resources defined in those manifests.  These are the general steps to building an app stack management operator with Operator Builder:</p> <ul> <li>Construct the Kubernetes manifests for the application you want to manage and   test them in a Kubernetes cluster.  You can also use Helm and the <code>helm template</code>   command to create these resources if a helm chart exists.</li> <li>Determine which fields in the manifests need to be mutable and managed by the   operator, then add markers to the manifests.</li> <li>Create a workload configuration to give it some details,   such as what you would like to call your custom resource.</li> <li>Run the Operator Builder CLI in a new repository and provide to it the marked   up manifests and workload config.</li> </ul> <p>That's it!  You will now have a Kubernetes operator that will create, update and delete the resources that constitute your application in response to creating, updating or deleting a custom resource instance.</p> <p>An operator built with Operator Builder has the following features:</p> <ul> <li>A defined API for a custom resource based on markers in   static Kubernetes manifests.</li> <li>A functioning controller that will create, update and delete child resources   to reconcile the state for the custom resource/s.</li> <li>A companion CLI that helps end users with common   operations.</li> </ul> <p>The custom resource defined in the source code can be cluster-scoped or namespace-scoped based on the requirements of the project.  More info here.</p>"},{"location":"#advanced-functionality","title":"Advanced Functionality","text":"<p>Advanced operational capabilities such as backups, restorations, failovers and upgrades are not included in an operator built by Operator Builder.  However, all the essential CRUD operations are generated for you, accelerating development and allowing you to get to the advanced functionality much faster.</p>"},{"location":"#built-atop-kubebuilder","title":"Built Atop Kubebuilder","text":"<p>Operator Builder is a Kubebuilder plugin.  Kubebuilder provides excellent scaffolding for Kubernetes operators but anyone who has built an app stack management operator using Kubebuilder can attest to the amount of time and effort required to define the managed resources in Go, not to mention the logic for creating, updating and deleting those resources. Operator Builder adds those resource definitions and other code to get you up and running in short order.</p>"},{"location":"#roadmap","title":"Roadmap","text":"<p>Please view our roadmap documentation to learn more about where we are headed and please open a new issue on GitHub if you have an idea or want to see something added to the roadmap.</p>"},{"location":"#license","title":"License","text":"<p>Copyright 2024 Nukleros and Third-party Authors and maintained by a core group of maintainers.</p> <p>The software is available under the Apache 2.0 License.</p>"},{"location":"api-updates-upgrades/","title":"API Updates &amp; Upgrades","text":"<p>This document deals with two things: 1. Updating an existing API. 2. Adding a new version of an API.</p>"},{"location":"api-updates-upgrades/#updating-an-existing-api","title":"Updating an Existing API","text":"<p>In this scenario, you are overwriting and changing an existing API specification. You should only do this during development with an unreleased API that no end user has started using.</p> <p>For example, you have begun development on a brand new operator.  You have generated the source code from a set of YAML manifests with markers.  While testing, you discover that a field is misspelled, or that a default value should be changed, or that a new field should be added.  The following instructions describe how to overwrite an existing API to update the existing spec.  Please  note that in the below example the <code>--resource=true</code> is not necessary and is  only provided in the example for verbosity.  This option is set by default.</p> <p>After making the necessary changes to your manifests run the following:</p> <pre><code>operator-builder create api \\\n    --workload-config [path/to/workload/config] \\\n    --controller=false \\\n    --resource=true \\\n    --force\n</code></pre> <p>You will pass the same workload config file.  The <code>--controller=false</code> flag will skip generating controller code but <code>--resource</code> and <code>--force</code> will cause the existing API to be overwritten.</p> <p>Note: if you change any of the following fields in the workload config you will get a new API rather than overwrite the existing one. - <code>spec.api.domain</code> - <code>spec.api.group</code> - <code>spec.api.version</code> - <code>spec.api.kind</code></p>"},{"location":"api-updates-upgrades/#adding-a-new-version-of-an-api","title":"Adding a New Version of an API","text":"<p>In this scenario, an existing version of an API is in use by end users.  You need to add features to the API that require adding, removing or altering fields.  Do not make breaking changes to an exising API that will render users' existing custom resource manifests invalid.</p> <p>Instead, you must add a new API version.  Now, you have two choices: 1. Maintain backward compatibility with previous API version/s. 2. Require an upgrade of API version with a new version of your controller and    companion CLI.  In this case, ensure your users understand what to expect so    they don't attempt to use an old API version with a new version of the    controller.</p>"},{"location":"api-updates-upgrades/#kubernetes-api-versions","title":"Kubernetes API Versions","text":"<p>Kubernetes has adopted detailed conventions for changing APIs.  To learn these details visit The Kubernetes API and API Overview in the Kubernetes docs.</p> <p>While you are naturally free to use your own conventions, we encourage you to model them on the Kubernetes system.  Your users will likely be familiar with them and those upstream conventions have sound reasoning behind them.</p> <p>For the purposes of this docuement, here are the important points: - Do not maintain backward compatibility for alpha API verions.  The development   cost of conversion between API versions is non-trivial.  Collaborate with the   early adopters that use your alpha versions and clearly document which   software versions support which API versions. - Maintain backward compatibility for beta versions of your APIs for 9 months or   3 releases (whichever is longer).  Do not release a beta version of your API   until you are confident few changes will be required in the forseeable future. - Maintain backward compatibility for stable versions for 12 months or 3   releases (whichever is longer).  Do not release a stable version until the API   is well tested and ready for production use.</p> <p>Following is a table that provides an example of what this may look like.  The first column shows the release version that applies to the controller and companion CLI.  The versions for these two components will always be pinned together - they share source code after all.  Notice that there is only one major version change.  The 1.0 release coincides with the v1 release of the API and signifies production readiness.  It does not signify any breaking change as the API versions have their own compatibility lifecycle.  As such there is no meaning to a 2.0 release of the controller and CLI.  This would only make sense if entirely new API groups and types were implemented to break backward compatibility.</p> Controller, CLI Version Supported API Versions Notes 0.1 v1alpha1 Initial Release 0.2 v1alpha2 v1alpha1 support removed 0.3 v1alpha3 v1alpha2 support removed 0.4 v1beta1 v1alpha3 support removed 0.5 v1beta2, v1beta1 v1beta1 deprecated 0.6 v1beta2, v1beta1 No change in API, software features added 1.0 v1, v1beta2 v1beta1 support removed, v1beta2 deprecated 1.1 v2alpha1, v1 1.2 v2alpha2, v1 v1beta2 and v2alpha1 support removed 1.3 v2beta1, v1 v2alpha2 support removed 1.4 v2beta2, v2beta1, v1 v2beta1 deprecated 1.5 v2, v2beta2, v2beta1, v1 v2beta2 and v1 deprecated 1.6 v2, v2beta2, v1 v2beta1 support removed 1.7 v2, v1 v2beta2 support removed 1.8 v2 v1 support removed <p>Note there there is no change in supported API versions with version 0.6.  The convention of increasing minor version when software features are added remains. Any time an API version is added or removed, a new minor version should be released, however an API change is not needed to justify a new minor version of the software when other features not related to an API are released.</p> <p>To create a new version of an existing API, update the <code>spec.api.version</code> value in your workload config, for example:</p> <pre><code>name: webstore\nkind: StandaloneWorkload\nspec:\n  api:\n    domain: acme.com\n    group: apps\n    version: v1alpha2  # existing API version is v1alapha1\n    kind: WebStore\n    clusterScoped: false\n  companionCliRootcmd:\n    name: webstorectl\n    description: Manage the webstore app\n  resources:\n  - app.yaml\n</code></pre> <p>Now reference the config in a new <code>create api</code> command:</p> <pre><code>operator-builder create api \\\n    --workload-config [path/to/workload/config] \\\n    --controller \\\n    --resource \\\n    --force\n</code></pre> <p>Note that we do want to re-generate the controller in this case. A new API definition will be create alongside the previous version.  If the earlier version of the API is to be unsupported, you can delete the earlier version.</p> <p>For example if your APIs look as follows:</p> <pre><code>tree apis/apps\napis/apps\n\u251c\u2500\u2500 v1alpha1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 groupversion_info.go\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 webstore\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 app.go\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 resources.go\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 webstore_types.go\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 zz_generated.deepcopy.go\n\u2514\u2500\u2500 v1alpha2\n    \u251c\u2500\u2500 groupversion_info.go\n    \u251c\u2500\u2500 webstore\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 app.go\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 resources.go\n    \u251c\u2500\u2500 webstore_types.go\n    \u2514\u2500\u2500 zz_generated.deepcopy.go\n\n4 directories, 10 files\n</code></pre> <p>You will delete the earlier version with <code>rm -rf apis/apps/v1alpha1</code>.</p> <p>If, however, you want to retain backward compatibility and support both versions you will need to implement conversion between the APIs.  Operator Builder does not yet support any scaffolding or code generation for this.  For details on how to accomplish this, refer to the Kubebuilder docs on API conversion.</p>"},{"location":"companion-cli/","title":"Companion CLI","text":"<p>When you generate the source code for a Kubernetes operator with Operator Builder, it can include the code for a companion CLI.  The source code for the companion CLI will be found in the <code>cmd</code> directory of the generated codebase.</p> <p>The companion CLI does three things: 1. Generate Sample Manifests: the <code>init</code> command will save a sample manifest to    disk for a custom resource.  This gives the end user a convenient way to get    started with defining configuration variables. 2. Generate Child Resource Manifests: the <code>generate</code> command prints the    manifests for all of the custom resources children - the Kubernetes resources    that are created and managed when a custom resource is created.  This offers    the end user a workaround when they need to configure changes that are not    exposed by the operator. 3. Install the Operator: the <code>install</code> command installs the operator, CRDs and    necessary resources in a Kubernetes cluster.</p> <p>These are the CLI configurations: 1. No CLI: Don't define any companion CLI data and no CLI source code will be    scaffolded. 2. A single root command: define the <code>spec.companionCliRootcmd</code> fields in a    standalone <code>WorkloadConfig</code> manifest. 3. A root command with subcommands: define the <code>spec.companionCliRootcmd</code> in a    collection <code>WorkloadConfig</code> manifest.  Then define <code>spec.companionCliSubcmd</code>    in one or more component <code>WorkloadConfig</code> manifests.</p>"},{"location":"companion-cli/#root-command","title":"Root Command","text":"<p>The root command for the CLI can be defined in a standalone workload or in a workload collection.</p>"},{"location":"companion-cli/#subcommands","title":"Subcommands","text":"<p>If a workload belongs to a collection you may define a subcommand for that workload.</p>"},{"location":"contact/","title":"Contact","text":"<p>If you find a bug or issue, please open an issue on Github</p> <p>If you would like to contribute to Operator Builder:</p> <ul> <li>Check out the Contributing   Guide</li> <li>For simple bug fixes or documentation updates, please open a   PR</li> <li>For new feature additions or involved changes, please open an   issue before you   start work.</li> </ul> <p>For general help or questions, hit us up in the operator-builder channel on Kubernetes Slack</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Make</li> <li>Go version 1.22 or later</li> <li>Docker (for building/pushing controller images)</li> <li>An available test cluster. A local kind or minikube cluster will work just   fine in many cases.</li> <li>Operator Builder installed.</li> <li>kubectl installed.</li> <li>A set of static Kubernetes manifests that can be used to deploy   your workload.  It is highly recommended that you apply these manifests to a   test cluster and verify the resulting resources work as expected.   If you don't have a workload of your own to use, you can use the examples   provided in this guide.</li> </ul>"},{"location":"getting-started/#guide","title":"Guide","text":"<p>This guide will walk you through the creation of a Kubernetes operator for a single workload.  This workload can consist of any number of Kubernetes resources and will be configured with a single custom resource.  Please review the prerequisites prior to attempting to follow this guide.</p> <p>This guide consists of the following steps:</p> <ol> <li>Create a repository.</li> <li>Determine what fields in your static manifests will need to be configurable for    deployment into different environments. Add commented markers to the    manifests. These will serve as instructions to Operator Builder.</li> <li>Create a workload configuration for your    project.</li> <li>Use the Operator Builder CLI to generate the source code for your operator.</li> <li>Test the operator against your test cluster.</li> <li>Build and install your operator's controller manager in your test cluster.</li> <li>Build and test the operator's companion    CLI.</li> </ol>"},{"location":"getting-started/#step-1-create-a-repo","title":"Step 1: Create a Repo","text":"<p>Create a new directory for your operator's source code.  We recommend you follow the standard code organization guidelines. In that directory initialize a new git repo.</p> <pre><code>git init\n</code></pre> <p>And intialize a new go module.  The module should be the import path for your project, usually something like <code>github.com/user-account/project-name</code>.  Use the command <code>go help importpath</code> for more info.</p> <pre><code>go mod init [module]\n</code></pre> <p>Lastly create a directory for your static manifests.  Operator Builder will use these as a source for defining resources in your operator's codebase.  It must be a hidden directory so as not to interfere with source code generation.</p> <pre><code>mkdir .source-manifests\n</code></pre> <p>Put your static manifests in this <code>.source-manifests</code> directory.  In the next step we will add commented markers to them.  Note that these static manifests can be in one or more files.  And you can have one or more manifests (separated by <code>---</code>) in each file.  Just organize them in a way that makes sense to you.</p>"},{"location":"getting-started/#step-2-add-manifest-markers","title":"Step 2: Add Manifest Markers","text":"<p>Look through your static manifests and determine which fields will need to be configurable for deployment into different environments.  Let's look at a simple example to illustrate.  Following is a Deployment, Ingress and Service that may be used to deploy a workload.</p> <pre><code># .source-manifests/app.yaml\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webstore-deploy\nspec:\n  replicas: 2                       # &lt;===== configurable\n  selector:\n    matchLabels:\n      app: webstore\n  template:\n    metadata:\n      labels:\n        app: webstore\n    spec:\n      containers:\n      - name: webstore-container\n        image: nginx:1.17           # &lt;===== configurable\n        ports:\n        - containerPort: 8080\n---\napiVersion: networking.k8s.io/v1beta1\nkind: Ingress\nmetadata:\n  name: webstore-ing\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\nspec:\n  rules:\n  - host: app.acme.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: webstorep-svc\n          servicePort: 80\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: webstore-svc\nspec:\n  selector:\n    app: webstore\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 8080\n</code></pre> <p>There are two fields in the Deployment manifest that will need to be configurable. They are noted with comments. The Deployment's replicas and the Pod's container image will change between different environments.  For example, in a dev environment the number of replicas will be low and a development version of the app will be run.  In production, there will be more replicas and a stable release of the app will be used. In this example we don't have any configurable fields in the Ingress or Service.</p> <p>Next we need to use <code>+operator-builder:field</code> markers in comments to inform Operator Builder that the operator will need to support configuration of these elements. Following is the Deployment manifest with these markers in place.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webstore-deploy\n  labels:\n    team: dev-team  # +operator-builder:field:name=teamName,type=string\nspec:\n  replicas: 2  # +operator-builder:field:name=webStoreReplicas,default=2,type=int\n  selector:\n    matchLabels:\n      app: webstore\n  template:\n    metadata:\n      labels:\n        app: webstore\n        team: dev-team  # +operator-builder:field:name=teamName,type=string\n    spec:\n      containers:\n      - name: webstore-container\n        image: nginx:1.17  # +operator-builder:field:name=webStoreImage,type=string\n        ports:\n        - containerPort: 8080\n</code></pre> <p>These markers should always be provided as an in-line comment or as a head comment.  The marker always begins with <code>+operator-builder:field:</code> or <code>+operator-builder:collection:field:</code> See Markers to learn more.</p>"},{"location":"getting-started/#step-3-create-a-workload-config","title":"Step 3: Create a Workload Config","text":"<p>Operator Builder uses a workload configuration to provide important details for your operator project.  This guide uses a standalone workload. Save a workload config to your <code>.source-manifests</code> directory by using one of the following commands (or simply copy/pasting the YAML below the commands):</p> <pre><code># generate a workload config with the path (-p) flag\noperator-builder init-config standalone -p .source-manifests/workload.yaml\n\n# generate a workload config from stdout\noperator-builder init-config standalone &gt; .source-manifests/workload.yaml\n</code></pre> <p>This will generate the following YAML:</p> <pre><code># .source-manifests/workload.yaml\nname: webstore\nkind: StandaloneWorkload\nspec:\n  api:\n    domain: acme.com\n    group: apps\n    version: v1alpha1\n    kind: WebStore\n    clusterScoped: false\n  companionCliRootcmd:\n    name: webstorectl\n    description: Manage webstore application\n  resources:\n  - app.yaml\n</code></pre> <p>The <code>name</code> is arbitrary and can be whatever you like.</p> <p>In the <code>spec</code>, the following fields are required:</p> <ul> <li><code>api.domain</code>: This must be a globally unique name that will not be used by other   organizations or groups.  It will contain groups of API types.</li> <li><code>api.group</code>: This is a logical group of API types used as a namespacing   mechanism for your APIs.</li> <li><code>api.version</code>: Provide the intiial version for your API.</li> <li><code>api.kind</code>: The name of the API type that will represent the workload you are   managing with this operator.</li> <li><code>resources</code>: An array of filenames where your static manifests live.  List the   relative path from the workload manifest to all the files that contain the   static manifests we talked about in step 2.</li> </ul> <p>For more info about API groups, versions and kinds, check out the Kubebuilder docs.</p> <p>The following fields in the <code>spec</code> are optional:</p> <ul> <li><code>api.clusterScoped</code>: If your workload includes cluster-scoped resources like   namespaces, this will need to be <code>true</code>.  The default is <code>false</code>.</li> <li><code>companionCLIRootcmd</code>: If you wish to generate source code for a companion CLI   for your operator, include this field.  We recommend you do.  Your end users   will appreciate it.</li> <li><code>name</code>: The root command your end users will type when using the companion     CLI.</li> <li><code>description</code>: The general information your end users will get if they use     the <code>help</code> subcommand of your companion CLI.</li> </ul> <p>At this point in our example, our <code>.source-manifests</code> directory looks as follows:</p> <pre><code>tree .source-manifests\n\n.source-manifests\n\u251c\u2500\u2500 app.yaml\n\u2514\u2500\u2500 workload.yaml\n</code></pre> <p>Our StandaloneWorkload config is in <code>workload.yaml</code> and the Deployment, Ingress and Service manifests are in <code>app.yaml</code> and referenced under <code>spec.resources</code> in our StandaloneWorkload config.</p> <p>We are now ready to generate our project's source code.</p>"},{"location":"getting-started/#step-4-generate-operator-source-code","title":"Step 4: Generate Operator Source Code","text":"<p>We first use the <code>init</code> command to create the general scaffolding.  We run this command from the root of our repo and provide a single argument with the path to our workload config.</p> <pre><code>operator-builder init \\\n    --workload-config .source-manifests/workload.yaml\n</code></pre> <p>With the basic project now set up, we can now run the <code>create api</code> command to create a new custom API for our workload.</p> <pre><code>operator-builder create api \\\n    --workload-config .source-manifests/workload.yaml \\\n    --controller \\\n    --resource\n</code></pre> <p>We again provide the same workload config file.  Here we also added the <code>--controller</code> and <code>--resource</code> arguments.  These indicate that we want both a new controller and new custom resource created.  Please  note that in the above example both flags are not necessary and are  only provided in the example for verbosity.  These options are set by default.</p> <p>You now have a new working Kubernetes Operator!  Next, we will test it out.</p>"},{"location":"getting-started/#step-5-run-test-the-operator","title":"Step 5: Run &amp; Test the Operator","text":"<p>Assuming you have a kubeconfig in place that allows you to interact with your cluster with kubectl, you are ready to go.</p> <p>First, install the new custom resource definition (CRD).</p> <pre><code>make install\n</code></pre> <p>Now we can run the controller locally to test it out.</p> <pre><code>make run\n</code></pre> <p>Operator Builder created a sample manifest in the <code>config/samples</code> directory. For this example it looks like this:</p> <pre><code>apiVersion: apps.acme.com/v1alpha1\nkind: WebStore\nmetadata:\n  name: webstore-sample\nspec:\n  webStoreReplicas: 2\n  webStoreImage: nginx:1.17\n  teamName: dev-team\n</code></pre> <p>You will notice the fields and values in the <code>spec</code> were derived from the markers you added to your static manifests.</p> <p>Next, in another terminal, create a new instance of your workload with the provided sample manifest.</p> <pre><code>kubectl apply -f config/samples/\n</code></pre> <p>You should see your custom resource sample get created.  Now use <code>kubectl</code> to inspect your cluster to confirm the workload's resources got created.  You should find all the resources that were defined in your static manifests.</p> <pre><code>kubectl get all\n</code></pre> <p>Clean up by stopping your controller with ctrl-c in that terminal and then remove all the resources you just created.</p> <pre><code>make uninstall\n</code></pre>"},{"location":"getting-started/#step-6-build-deploy-the-controller-manager","title":"Step 6: Build &amp; Deploy the Controller Manager","text":"<p>Now let's deploy your controller into the cluster.</p> <p>First export an environment variable for your container image.</p> <pre><code>export IMG=myrepo/acme-webstore-mgr:0.1.0\n</code></pre> <p>Run the rest of the commands in this step 6 in this same terminal as most of them will need this <code>IMG</code> env var.</p> <p>In order to run the controller in-cluster (as opposed to running locally with <code>make run</code>) we will need to build a container image for it.</p> <pre><code>make docker-build\n</code></pre> <p>Now we can push it to a registry that is accessible from the test cluster.</p> <pre><code>make docker-push\n</code></pre> <p>Finally, we can deploy it to our test cluster.</p> <pre><code>make deploy\n</code></pre> <p>Next, perform the same tests from step 5 to ensure proper operation of our operator.</p> <pre><code>kubectl apply -f config/sample/\n</code></pre> <p>Again, verify that all the resources you expect are created.</p> <p>Once satisfied, remove the instance of your workload.</p> <pre><code>kubectl delete -f config/sample/\n</code></pre> <p>For now, leave the controller running in your test cluster.  We'll use it in Step 7.</p>"},{"location":"getting-started/#step-7-build-test-companion-cli","title":"Step 7: Build &amp; Test Companion CLI","text":"<p>Now let's build and test the companion CLI.</p> <p>You will have a make target that includes the name of your CLI.  For this example it is:</p> <pre><code>make build-webstorectl\n</code></pre> <p>We can view the help info as follows.</p> <pre><code>./bin/webstorectl help\n</code></pre> <p>Your end users can use it to create a new custom resource manifest.</p> <pre><code>./bin/webstorectl init &gt; /tmp/webstore.yaml\n</code></pre> <p>If you would like to change any of the default values, edit the file.</p> <pre><code>vim /tmp/webstore.yaml\n</code></pre> <p>Then you can apply it to the cluster.</p> <pre><code>kubectl apply -f /tmp/webstore.yaml\n</code></pre> <p>If your end users find they wish to make changes to the resources that aren't supported by the operator, they can generate the resources from the custom resource.</p> <pre><code>./bin/webstorectl generate --workload-manifest /tmp/webstore.yaml\n</code></pre> <p>This will print the resources to stdout.  These may be piped into an overlay tool or written to disk and modified before applying to a cluster.</p> <p>That's it!  You have a working operator without manually writing a single line of code.  If you'd like to make any changes to your workload's API, you'll find the code in the <code>apis</code> directory.  The controller's source code is in <code>controllers</code> directory.  And the companion CLI code is in <code>cmd</code>.</p> <p>Don't forget to clean up.  Remove the controller, CRD and the workload's resources as follows.</p> <pre><code>make undeploy\n</code></pre>"},{"location":"getting-started/#next-step","title":"Next Step","text":"<p>Learn about Workloads.</p>"},{"location":"installation/","title":"Installation","text":"<p>You have the following options to install the operator-builder CLI:</p> <ul> <li>Download the latest binary with your browser</li> <li>Download with wget</li> <li>Homebrew</li> <li>Docker Image</li> <li>Go Install</li> </ul>"},{"location":"installation/#wget","title":"wget","text":"<p>Use wget to download the pre-compiled binaries:</p> <pre><code>VERSION=v0.11.0\nOS=Linux\nARCH=x86_64\nwget https://github.com/nukleros/operator-builder/releases/download/${VERSION}/operator-builder_${VERSION}_${OS}_${ARCH}.tar.gz -O - |\\\n    tar -xz &amp;&amp; sudo mv operator-builder /usr/local/bin/operator-builder\n</code></pre>"},{"location":"installation/#homebrew","title":"Homebrew","text":"<p>Available for Mac and Linux.</p> <p>Using Homebrew</p> <pre><code>brew tap nukleros/tap\nbrew install nukleros/tap/operator-builder\n</code></pre>"},{"location":"installation/#docker-image","title":"Docker Image","text":"<pre><code>docker pull ghcr.io/nukleros/operator-builder\n</code></pre>"},{"location":"installation/#one-shot-container-use","title":"One-shot container use","text":"<pre><code>docker run --rm -v \"${PWD}\":/workdir ghcr.io/nukleros/operator-builder [flags]\n</code></pre>"},{"location":"installation/#run-container-commands-interactively","title":"Run container commands interactively","text":"<pre><code>docker run --rm -it -v \"${PWD}\":/workdir --entrypoint sh ghcr.io/nukleros/operator-builder\n</code></pre> <p>It can be useful to have a bash function to avoid typing the whole docker command:</p> <pre><code>operator-builder() {\n  docker run --rm -i -v \"${PWD}\":/workdir ghcr.io/nukleros/operator-builder \"$@\"\n}\n</code></pre>"},{"location":"installation/#go-install","title":"Go Install","text":"<pre><code>go install github.com/nukleros/operator-builder/cmd/operator-builder@latest\n</code></pre>"},{"location":"license/","title":"License Management","text":"<p>Manage the creation and update of licensing for your Kubebuilder project.</p>"},{"location":"license/#try-it-out","title":"Try it Out","text":"<p>Create two license files for testing:</p> <pre><code>cat &gt; /tmp/project.txt &lt;&lt;EOF\n    MIT License\n\n    Copyright (c) Acme Inc. All rights reserved.\n\n    Permission is hereby granted, free of charge, to any person obtaining a copy\n    of this software and associated documentation files (the \"Software\"), to deal\n    in the Software without restriction, including without limitation the rights\n    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n    copies of the Software, and to permit persons to whom the Software is\n    furnished to do so, subject to the following conditions:\n\n    The above copyright notice and this permission notice shall be included in all\n    copies or substantial portions of the Software.\n\n    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n    SOFTWARE\n\nEOF\n\ncat &gt; /tmp/source-header.txt &lt;&lt;EOF\n// Copyright 2006-2021 Acme Inc.\n// SPDX-License-Identifier: Apache-2.0\nEOF\n</code></pre> <p>Now initialize a new Kubebuilder project and reference your license files.</p> <pre><code>operator-builder init \\\n    --domain apps.acme.com \\\n    --project-license /tmp/project.txt \\\n    --source-header-license /tmp/source-header.txt\n</code></pre> <p>You will now have a <code>LICENSE</code> file in your project which has the contents of <code>/tmp/project.txt</code>.  The <code>hack/boilerplate.go.txt</code> file will have the contents of <code>/tmp/source-header.txt</code> which you will also find at the top of <code>main.go</code>.</p>"},{"location":"license/#update-existing-project","title":"Update Existing Project","text":"<p>If you have an existing project that you would like to update licensing for:</p> <pre><code>operator-builder update license \\\n    --project-license /tmp/project.txt \\\n    --source-header-license /tmp/source-header.txt\n</code></pre>"},{"location":"markers/","title":"Markers","text":"<p>Operator Builder uses commented markers as the basis for defining a new API. The fields for a custom resource kind are created when it finds a <code>+operator-builder</code> marker in a source manifest.</p> <p>A workload marker is commented out so the manifest is still valid and can be used if needed.  The marker must begin with <code>+operator-builder</code> followed by some colon-separated fields:</p> <p>These markers should always be provided as an in-line comment or as a head comment.  The marker always begins with <code>+operator-builder:field:</code> or <code>+operator-builder:collection:field:</code> (more on this later).</p> <p>That is followed by arguments separated by <code>,</code>.  Arguments can be given in any order.</p>"},{"location":"markers/#arguments","title":"Arguments","text":"<p>Arguments come after the actual marker and are separated from the marker name  with a <code>:</code>. They are given in the format of <code>argument=value</code> and separated by  the <code>,</code>. Additionally, if the argument name is given by itself with no value, it  is assumed to have an implict <code>=true</code> on the end and is treated as a flag.</p> <p>Below you will find the supported markers and their supported arguments.</p>"},{"location":"markers/#field-markers","title":"Field Markers","text":"<p>Defined as <code>+operator-builder:field</code> this marker can be used to define a CRD field for your workload.</p> Field Type Required name string true type string{string, int, bool} true default type false replace string false arbitrary bool false description string false"},{"location":"markers/#name-required-if-parent-is-unspecified","title":"Name (required if Parent is unspecified)","text":"<p>The name you want to use for the field in the custom resource that Operator Builder will create.  If you're not sure what that means, it will become clear shortly.</p> <p>Example:</p> <pre><code>+operator-builder:field:name=myName,type=string\n</code></pre>"},{"location":"markers/#parent-required-if-name-is-unspecified","title":"Parent (required if Name is unspecified)","text":"<p>The parent field in which you wish to substitute.  Currently, only <code>metadata.name</code> is supported. This will allow you to use the parent name as a value in the child resource.</p> <p>Example:</p> <pre><code>+operator-builder:field:parent=metadata.name,type=string\n</code></pre> <p>The <code>metadata.name</code> field from the collection workload is also supported:</p> <pre><code>+operator-builder:collection:field:parent=metadata.name,type=string\n</code></pre>"},{"location":"markers/#type-required","title":"Type (required)","text":"<p>The other required field is the <code>type</code> field which specifies the data type for the value.</p> <p>The supported data types are:</p> <ul> <li>bool</li> <li>string</li> <li>int</li> <li>int32</li> <li>int64</li> <li>float32</li> <li>float64</li> </ul> <p>ex. <code>+operator-builder:field:name=myName,type=string</code></p>"},{"location":"markers/#default-optional","title":"Default (optional)","text":"<p>This will make configuration optional for your operator's end user. the supplied value will be used for the default value. If a field has no default, it will be a required field in the custom resource.  For example:</p> <pre><code>+operator-builder:field:name=myName,type=string,default=test\n</code></pre>"},{"location":"markers/#replace-optional","title":"Replace (optional)","text":"<p>There may be some instances where you only want a specific portion of a value to be configurable (such as config maps). In these scenarios you can use the replace argument to specify a search string (or regex) to target for configuration.</p> <p>Consider the following example:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    # +operator-builder:field:name=environment,default=dev,type=string,replace=\"dev\"\n    app: myapp-dev\n  name: contour-configmap\n  namespace: ingress-system\ndata:\n  # +operator-builder:field:name=configOption,default=myoption,type=string,replace=\"configuration2\"\n  # +operator-builder:field:name=yamlType,default=myoption,type=string,replace=\"multi.*yaml\"\n  config.yaml: |\n    ---\n    someoption: configuration2\n    anotheroption: configuration1\n    justtesting: multistringyaml\n</code></pre> <p>In this scenario three custom resource fields will be generated. The value from the <code>environment</code> field will replace the <code>dev</code> portion of <code>myapp-dev</code>. For example, if <code>prod</code> is provided as a value for the <code>environment</code> field, the resulting config map will get the label <code>app: myapp-prod</code>. Values from the <code>configOption</code> and <code>yamlType</code> fields will replace corresponding strings in the content of <code>config.yaml</code>.  The resulting configmap will look as follows:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    app: myapp-prod\n  name: contour-configmap\n  namespace: ingress-system\ndata:\n  config.yaml: |-\n    ---\n    someoption: myoption\n    anotheroption: configuration1\n    justtesting: myoption\n</code></pre>"},{"location":"markers/#arbitrary-optional","title":"Arbitrary (optional)","text":"<p>If you wish to create a field for a custom resource that does not directly map to a value in a child resource, mark a field as arbitrary.</p> <p>Here is an example of how to mark a field as arbitrary:</p> <pre><code>---\n# +operator-builder:field:name=nginx.installType,arbitrary,default=\"deployment\",type=string,description=`\n# +kubebuilder:validation:Enum=deployment;daemonset\n# Method of install nginx ingress controller.  One of: deployment | daemonset.`\napiVersion: v1\nkind: Namespace\nmetadata:\n  # +operator-builder:field:name=namespace,default=\"nukleros-ingress-system\",type=string,description=`\n  # Namespace to use for ingress support services.`\n  name: nukleros-ingress-system\n</code></pre> <p>On the first line you can see the <code>nginx.installType</code> custom resource field is marked as arbitrary with the <code>arbitrary</code> marker field.  Where you place this marker is unimportant but it is recommended you put all arbitrary fields at the beginning of one chosen manifest for ease of maintenance.</p> <p>This will result in a custom resource sample that looks as follows:</p> <pre><code>apiVersion: platform.addons.nukleros.io/v1alpha1\nkind: IngressComponent\nmetadata:\n  name: ingresscomponent-sample\nspec:\n  #collection:\n    #name: \"supportservices-sample\"\n    #namespace: \"\"\n  nginx:\n    installType: \"deployment\"            # &lt;---- arbitary field\n    image: \"nginx/nginx-ingress\"\n    version: \"2.3.0\"\n    replicas: 2\n  namespace: \"nukleros-ingress-system\"\n</code></pre> <p>This arbitrary field will not map to any child resource value.  However it can be leveraged by some custom mutation code or by a resource marker such as this:</p> <pre><code>---\n# +operator-builder:resource:field=nginx.installType,value=\"deployment\",include\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nukleros-ingress-system # +operator-builder:field:name=namespace,default=\"nukleros-ingress-system\",type=string\n...\n</code></pre> <p>The marker on line one indicates the deployment resource only be created if <code>nginx.installType</code> has a value of <code>deployment</code> (as shown in the custom resource sample above).  In this example, we are providing an option to install the Nginx Ingress Controller as a deployment or a daemonset.</p>"},{"location":"markers/#description-optional","title":"Description (optional)","text":"<p>An optional description can be provided which will be used in the source code as a Doc String, backticks <code>`</code> may be used to capture multiline strings (head comments only).</p> <p>By injecting documentation to the CRD, the consumer of the custom resource gets the added benefit by being able to run <code>kubectl explain</code> against their resource and having documentation right at their fingertips without having to navigate to API documentation in order to see the usage of the API.  For example:</p> <pre><code>+operator-builder:field:name=myName,type=string,default=test,description=\"Hello World\"\n</code></pre> <p>Note: that you can use a single custom resource field name to configure multiple fields in the resource.</p> <p>Consider the following Deployment:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: webapp-deploy\n  labels:\n    production: false  # +operator-builder:field:name=production,default=false,type=bool\nspec:\n  replicas: 2  # +operator-builder:field:name=webAppReplicas,default=2,type=int\n  selector:\n    matchLabels:\n      app: webapp\n  template:\n    metadata:\n      labels:\n        app: webapp\n    spec:\n      containers:\n      - name: webapp-container\n        image: nginx:1.17  # +operator-builder:field:name=webAppImage,type=string\n        ports:\n        - containerPort: 8080\n</code></pre> <p>In this case, operator-builder will create and add three fields to the custom resource:</p> <ul> <li>A <code>production</code> field that is a boolean.  It will have a default of <code>false</code> and   will inform the value of the label when the deployment is configured.</li> <li>A <code>webAppReplicas</code> field that will default to <code>2</code> and allow the user to   specify the number of replicas for the deployment in the custom resource   manifest.</li> <li>A <code>webAppImage</code> field that will set the value for the images used in the pods.</li> </ul> <p>Now the end-user of the operator will be able to define a custom resource similar to the following to configure the deployment created:</p> <pre><code>apiVersion: product.apps.acme.com/v1alpha1\nkind: WebApp\nmetadata:\n  name: dev-webapp\nspec:\n  production: false\n  webAppReplicas: 2\n  webAppImage: acmerepo/webapp:3.5.3\n</code></pre>"},{"location":"markers/#collection-markers","title":"Collection Markers","text":"<p>A second marker type <code>+operator-builder:collection:field</code> can be used with the same arguments as a Field Marker. These markers are used to define global fields for your Collection and can be used in any of its associated components.</p> <p>If you include any marker on a collection resource it will be treated as a collection marker and will configure a field in the collection's custom resource.</p>"},{"location":"markers/#resource-markers","title":"Resource Markers","text":"<p>Defined as <code>+operator-builder:resource</code> this marker can be used to control a specific resource with arguments in the marker.</p> <p>Note: a resource marker must reference a field defined by a field marker.  If you include a resource marker with a unique field name that is not also defined by a field marker you will get an error.  You may use an arbitrary field on a field marker if you don't wish to associate the field with a value in a child resource.</p> Field Type Required field string true collectionField string{string, int, bool} true value type true include bool true"},{"location":"markers/#field-collectionfield-required","title":"Field / CollectionField (required)","text":"<p>The conditional field to associate with an action (currently only include). One of <code>field</code> or <code>collectionField</code> must be provided depending upon if you are checking a condition against a collection, or a component/standalone workload spec. The field input relates directly to a given workload marker such as <code>+operator-builder:field:name=provider</code> would produce a field of <code>provider</code> to be used in a resource marker with argument <code>field=provider</code>.</p> <p>ex. +operator-builder:resource:collectionField=provider,value=\"aws\",include ex. +operator-builder:resource:field=provider,value=\"aws\",include=false</p>"},{"location":"markers/#value-required","title":"Value (required)","text":"<p>The conditional value to associate with an action (currently only <code>include</code> - see above).  The <code>value</code> input relates directly to the value of <code>field</code> as it exists in the API spec requested by the user.</p> <p>Examples:</p> <pre><code>+operator-builder:resource:collectionField=provider,value=\"aws\",include\n+operator-builder:resource:field=provider,value=\"aws\",include=false\n</code></pre>"},{"location":"markers/#include-required","title":"Include (required)","text":"<p>The action to perform on the resource.  Include will include the resource for deployment during a control loop given a <code>field</code> or <code>collectionField</code> and a <code>value</code>. Using this means that the resource will only be included if this condition is met.  If the condition is not met, the resource will not be deployed.</p> <p>Here are some sample marker examples:</p> <pre><code>+operator-builder:resource:field=provider,value=\"aws\",include\n+operator-builder:resource:field=provider,value=\"aws\",include=true\n+operator-builder:resource:collectionField=provider,value=\"aws\",include\n+operator-builder:resource:collectionField=provider,value=\"aws\",include=true\n</code></pre> <p>With include set to <code>false</code>, the opposite is true and the resource is excluded from being deployed during a control loop if a condition is met:</p> <p>Examples:</p> <pre><code>+operator-builder:resource:field=provider,value=\"aws\",include=false\n+operator-builder:resource:collectionField=provider,value=\"aws\",include=false\n</code></pre> <p>At this time, the <code>include</code> argument with <code>field</code> and <code>value</code> can be simply thought of as (pseudo-code):</p> <pre><code>if field == value {\n  if include {\n    includeResource()\n  }\n}\n</code></pre> <p>IMPORTANT: A resource marker is not required and should only be used when there is a desire to act upon a resource.  If no resource marker is provided, a resource is always deployed during a control loop.</p>"},{"location":"markers/#include-resource-on-condition","title":"Include Resource On Condition","text":"<p>Below is a sample of how to include a resource only if a condition is met.  If the condition is not met, the resource is not deployed during the control loop:</p> <pre><code># +operator-builder:resource:field=provider,value=\"aws\",include\n---\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: aws-storage-class\n  annotations:\n    storageclass.kubernetes.io/is-default-class: true\n  labels:\n    provider: \"aws\" # +operator-builder:field:name=provider,type=string,default=\"aws\"\nallowVolumeExpansion: false\nprovisioner: \"kubernetes.io/aws-ebs\"\nreclaimPolicy: \"Delete\"\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  type: \"gp2\"\n  iopsPerGB: 10\n  fsType: \"ext4\"\n  encrypted: false\n</code></pre> <p>Given the below CRD, the resource would be included:</p> <pre><code>apiVersion: apps.acme.com/v1alpha1\nkind: Sample\nmetadata:\n  name: sample\n  namespace: default\nspec:\n  provider: \"aws\"\n</code></pre> <p>Given the below CRD, the resource would NOT be included:</p> <pre><code>apiVersion: apps.acme.com/v1alpha1\nkind: Sample\nmetadata:\n  name: sample\n  namespace: default\nspec:\n  provider: \"azure\"\n</code></pre>"},{"location":"markers/#exclude-resource-on-condition","title":"Exclude Resource On Condition","text":"<p>Below is a sample of how to exclude a resource only if a condition is met.  If the condition is not met, the resource is not deployed during the control loop:</p> <pre><code># +operator-builder:resource:field=provider,value=\"azure\",include=false\n---\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: aws-storage-class\n  annotations:\n    storageclass.kubernetes.io/is-default-class: true\n  labels:\n    provider: \"aws\" # +operator-builder:field:name=provider,type=string,default=\"aws\"\nallowVolumeExpansion: false\nprovisioner: \"kubernetes.io/aws-ebs\"\nreclaimPolicy: \"Delete\"\nvolumeBindingMode: WaitForFirstConsumer\nparameters:\n  type: \"gp2\"\n  iopsPerGB: 10\n  fsType: \"ext4\"\n  encrypted: false\n</code></pre> <p>Given the below CRD, the resource would be included:</p> <pre><code>apiVersion: apps.acme.com/v1alpha1\nkind: Sample\nmetadata:\n  name: sample\n  namespace: default\nspec:\n  provider: \"aws\"\n</code></pre> <p>Given the below CRD, the resource would NOT be included:</p> <pre><code>apiVersion: apps.acme.com/v1alpha1\nkind: Sample\nmetadata:\n  name: sample\n  namespace: default\nspec:\n  provider: \"azure\"\n</code></pre>"},{"location":"markers/#stacking-resource-markers","title":"Stacking Resource Markers","text":"<p>You can include multiple resource markers on a particular resource.  For example: <pre><code>---\n# +operator-builder:resource:field=nginx.include,value=true,include\n# +operator-builder:resource:field=nginx.installType,value=\"deployment\",include\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n...\n</code></pre></p> <p>The purpose of the first marker is to include all nginx ingress contoller resources when <code>spec.nginx.include: true</code>.  The second gives users a choice to install nginx ingress controller as a deployment or daemonset.  When <code>spec.nginx.installType: deployment</code> the deployment resource is included. Therefore the custom resource will need to look as follows for this deployment resource to be created:</p> <pre><code>apiVersion: platform.addons.nukleros.io/v1alpha1\nkind: IngressComponent\nmetadata:\n  name: ingresscomponent-sample\nspec:\n  nginx:\n    installType: \"deployment\"  # if not \"deployment\" deployment resource excluded\n    include: true  # if false, no nginx resources are created\n    image: \"nginx/nginx-ingress\"\n    version: \"2.3.0\"\n    replicas: 2\n</code></pre> <p>The resulting source code looks as follows.  If either if-statement is evaluated as true, the function will return without any object - hence the deployment will not be included.</p> <pre><code>// CreateDeploymentNamespaceNginxIngress creates the Deployment resource with name nginx-ingress.\nfunc CreateDeploymentNamespaceNginxIngress(\n    parent *platformv1alpha1.IngressComponent,\n    collection *setupv1alpha1.SupportServices,\n    reconciler workload.Reconciler,\n    req *workload.Request,\n) ([]client.Object, error) {\n\n    if parent.Spec.Nginx.Include != true {\n        return []client.Object{}, nil\n    }\n\n    if parent.Spec.Nginx.InstallType != \"deployment\" {\n        return []client.Object{}, nil\n    }\n\n    var resourceObj = &amp;unstructured.Unstructured{\n        Object: map[string]interface{}{\n            // +operator-builder:resource:field=nginx.include,value=true,include\n            // +operator-builder:resource:field=nginx.installType,value=\"deployment\",include\n            \"apiVersion\": \"apps/v1\",\n            \"kind\":       \"Deployment\",\n      ...\n            }\n\n    return mutate.MutateDeploymentNamespaceNginxIngress(resourceObj, parent, collection, reconciler, req)\n}\n</code></pre>"},{"location":"resource-scope/","title":"Resource Scope","text":"<p>Kubernetes resources are either cluster-scoped or namespace scoped.  The custom resources (CRs) created with workload-api can be either.  By default, they are namespace-scoped.  If you wish to create a cluster-scoped CR, you will need to specify in the WorkloadConfig manifest as shown in this example:</p> <pre><code>name: webapp\nspec:\n  group: apps\n  version: v1alpha1\n  kind: WebApp\n  clusterScoped: false  # &lt;-- indicates custom resource should be cluster-scoped\n  companionCliRootcmd:\n    name: webappctl\n    description: Manage webapp stuff like a boss\n  resources:\n  - app.yaml\n</code></pre> <p>In general, you will want to use the default namespace-scoped CR unless your CR will be a parent for cluster-scoped resources, e.g. namespaces or CRDs.</p> <p>NOTE: The scope of your CR will have a bearing on the the namespace for your CR's child resources, i.e. the resources that are owned and configured by your CR. 1. Namespace-scoped: If your CR is namespace-scoped, all child resources will be    created in the namespace your CR is created in.  In this case the lifecycle    of the namespace is managed outside of your operator. 2. Cluster-scoped: If your CR is cluster-scoped, you will need to include a    <code>metadata.namespace</code> field in all applicable source manifests and include a    field in your CR to set that value.  This is because your CR will not have a namespace    and so the namespace must be assigned by the operator.  In this case the    lifecycle of the namespace may be managed by your operator.</p>"},{"location":"standalone-workloads/","title":"Standalone Workloads","text":"<p>When you are building an operator for a single workload, you will just need a single standalone WorkloadConfig along with the source manifests to define the resources that will be created to fulfill an instance of your workload.</p> <p>For example if your organization develops and maintains a web application as a part of its core business, you may consider using an operator to deploy and maintain that app in different environments.  We refer to all the various resources that comprise that web app collectively as a \"workload.\"</p> <p>This is the simplest and most common implemetation of operator-builder.</p> <p>If you have multiple workloads that have dependencies upon one another, and it makes sense to orchestrate them with an operator, a standalone workload will not suffice.  For that you will need to leverage a workload collection.</p>"},{"location":"workload-collections/","title":"Workload Collections","text":"<p>If you are building an operator to manage a collection of workloads that have dependencies upon one another, you will need to use a workload collection.  If, instead, you have just a single workload to manage, you will want to use a standalone workload.</p> <p>A workload collection is defined by a configuration of kind <code>WorkloadCollection</code>. The only differences are that it will include an array of children <code>ComponentWorkloads</code> which are references to the individual, managed workloads via the <code>componentFiles</code> field.</p> <pre><code>name: acme-app-platform\nkind: WorkloadCollection\nspec:\n  api:\n    domain: apps.acme.com\n    group: platform\n    version: v1alpha1\n    kind: AcmeAppPlatform\n    clusterScoped: true\n  companionCliRootcmd:\n    name: platformctl\n    description: Manage app platform services like a boss\n  componentFiles:\n    - ingress-workload.yaml\n    - metrics-worklaod.yaml\n    - logging-workload.yaml\n    - admission-control-workload.yaml\n</code></pre> <p>Each of the <code>componentFiles</code> are <code>ComponentWorkload</code> configs that may have dependencies upon one another which the operator will manage as identified by the <code>dependencies</code> field (see below).  This project will include a custom resource for each of the component workloads as well as a distinct controller for each component workload.  Each of these controllers will run in a single containerized controller manager in the cluster when it is deployed.</p> <p>This collection WorkloadConfig includes a root command that will be used as a companion CLI for the Acme App Platform operator.  Each of the component workloads may also include a subcommand for use with that component.  It will look something like this:</p> <pre><code>name: metrics-component\nkind: ComponentWorkload\nspec:\n  api:\n    group: platform\n    version: v1alpha1\n    kind: MetricsComponent\n    clusterScoped: false\n  companionCliSubcmd:\n    name: metrics\n    description: Manage metrics in for the app platform like a boss\n  dependencies:\n    - ingress-component\n  resources:\n    - prom-operator.yaml\n    - prometheus.yaml\n    - alertmanager.yaml\n    - grafana.yaml\n    - prom-adapter.yaml\n    - kube-state-metrics.yaml\n    - node-exporter.yaml\n</code></pre> <p>Now the end uers of this operator - the platform operators - will be able to use the companion CLI and issue commands like <code>platformctl metrics init</code> to initialize a new MetricsComponent custom resource to use to deploy the metrics-component workload.  Or they can use <code>platformctl metrics generate</code> to ouptut a set of Kubernetes manifests configured by a supplied MetricsComponent custom resource.</p>"},{"location":"workload-collections/#collection-resources","title":"Collection Resources","text":"<p>Collections can also have resources associated directly with them.  This is useful for resources such as namespaces that are shared by the components in the collection.  Consider the following workload collection with a resource included.</p> <pre><code>name: acme-complex-app\nkind: WorkloadCollection\nspec:\n  api:\n    domain: apps.acme.com\n    group: tenant\n    version: v1alpha1\n    kind: AcmeComplexApp\n    clusterScoped: true\n  companionCliRootcmd:\n    name: appctl\n    description: Manage a really complex app\n  resources:\n    - namespace.yaml  # collection resource identified here\n  componentFiles:\n    - frontend-component.yaml\n    - backend-component.yaml\n    - service-x-component.yaml\n    - service-y-component.yaml\n    - service-z-component.yaml\n</code></pre> <p>Any markers included in these collection resources will configure fields for the collection's custom resource.  For example the following marker will result in a <code>namespace</code> field being included in the spec for a <code>AcmeComplexApp</code> resource:</p> <pre><code># namespace.yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: complex-app # +operator-builder:field:name=namespace,type=string\n</code></pre> <p>If you want to add leverage that same <code>namespace</code> field from the <code>AcmeComplexApp</code> field in a component resources ensure that you use a collection marker.  In the following example this component service resource will derive it's namespace from the <code>AcmeComplexApp</code> <code>spec.namespace</code> field.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: frontend-svc\n  namespace: complex-app # +operator-builder:collection:field:name=namespace,type=string\nspec:\n  ports:\n    - name: https\n      port: 443\n      protocol: TCP\n  selector:\n    app: frontend\n</code></pre>"},{"location":"workload-collections/#next-step","title":"Next Step","text":"<p>Follow the workload collection tutorial.</p>"},{"location":"workloads/","title":"Workloads","text":"<p>Operator Builder uses WorkloadConfig manifests to define \"workloads.\"  A workload represents any group of Kubernetes resources.  For example, a workload could be an application with Deployment, Service and Ingress resources.  A workload may also be a group of resources that provide a platform service.  They can be any group of resources that logically group together.  Here's an example of a simple WorkloadConfig for a hypothetical web application called \"webapp\":</p> <pre><code>name: webapp\nkind: StandaloneWorkload\nspec:\n  api:\n    domain: apps.acme.com\n    group: product\n    version: v1alpha1\n    kind: WebApp\n    clusterScoped: false\n  companionCliRootcmd:\n    name: webappctl\n    description: Manage webapp stuff like a boss\n  resources:\n    - deploy.yaml\n    - service.yaml\n    - ingress.yaml\n</code></pre> <p>This tells Operator Builder to create the source code for a new Kubernetes API called \"WebApp.\"  This is the API kind.  The resources that comprise this webapp are a deployment, service and ingress.  Those resources are defined in the source manifest files referenced under <code>resources</code>.  Operator Builder uses those source manifests to generate source code for those resources.  Those source manifests can contain markers to help define the API.</p> <p>With the source code generated, the companion CLI can be built for end users.  The controller container image can also be built and made available to end users.</p> <p>End users then use the CLI <code>install</code> command to deploy the operator and then use the <code>init</code> command to generate a sample custom resource that might look something like this:</p> <pre><code>apiVersion: product.apps.acme.com/v1alpha1\nkind: WebApp\nmetadata:\n  name: dev-webapp\nspec:\n  production: false\n  webAppReplicas: 2\n  webAppImage: acmerepo/webapp:3.5.3\n</code></pre>"},{"location":"workloads/#required-fields","title":"Required Fields","text":"<p>The following are required fields: - spec.api.domain   # required for 'operator-builder init' - spec.api.group    # required for 'operator-builder create api' - spec.api.version  # required for 'operator-builder create api' - spec.api.kind     # required for 'operator-builder create api'</p> <p>All other fields are optional.  The default value for <code>clusterScoped</code> if not defined is <code>false</code>.  Alternatively, the above fields can be defined imperatively via the <code>domain</code>, <code>group</code>, <code>version</code>, and <code>kind</code> flags when running either <code>operator-builder init</code> or <code>operater-builder create api</code> (see above for correct context).</p>"},{"location":"workloads/#resources","title":"Resources","text":"<p>When specifying resource manifest files under <code>spec.resources</code>, in addition to providing specific filenames, you may use glob pattern matching to collect files. For example:</p> <pre><code>name: webapp\nkind: StandaloneWorkload\nspec:\n  api:\n    domain: apps.acme.com\n    group: product\n    version: v1alpha1\n    kind: WebApp\n    clusterScoped: false\n  companionCliRootcmd:\n    name: webappctl\n    description: Manage webapp stuff like a boss\n  resources:\n    - rbac/*.yaml       # get all .yaml files in the rbac directory\n    - workload/**.yaml  # get all .yaml files recursively in the workload\n                        # directory and subdirectories therein\n</code></pre>"},{"location":"workloads/#collections","title":"Collections","text":"<p>The <code>spec.componentFiles</code> field can only be defined in a <code>WorkloadCollection</code>. See workload collections for more information.</p>"},{"location":"dev/","title":"Developer Docs","text":"<p>The documentation here is for developers of Operator Builder.</p> <p>If you're developing operators using Operator Builder see our user documentation.</p>"},{"location":"dev/testing/","title":"Testing","text":"<p>When making changes to Operator Builder, you can test your changes with functional tests that generate the codebase for a new operator.  You can also use delve to test changes.</p> <p>Keep in mind you are testing a source code generator that end-user engineers will use to manage Kubernetes operator projects. Operator Builder is used to generate code for a distinct code repository - so the testing is conducted as such.  It stamps out and/or modifies source code for an operator project when a functional or debug test is run.</p> <p>At this time, manual verification of results is needed.  In future, automated integration tests will be added to test the generated operator.</p>"},{"location":"dev/testing/#make-targets","title":"Make Targets","text":"<ul> <li><code>test</code>: Reserved for unit tests.</li> <li><code>test-e2e</code>: Run the E2E tests as indicated by the <code>e2e_test</code> tag and stored in the   <code>test/e2e</code> path from the generated repository.</li> <li><code>debug</code>: Runs operator-builder to generate an operator codebase using the   delve debugger.</li> <li><code>debug-clean</code>: Use with caution. Deletes the contents of <code>TEST_WORKLOAD_PATH</code>   where the test operator codebase is generated.</li> <li><code>func-test</code>: Runs a functional test of operator-builder.  It builds and uses   the built binary to run <code>operator-builder init</code> and <code>operator-builder create api</code>.   It generates a new operator codebase in <code>FUNC_TEST_PATH</code>.</li> <li><code>func-test-clean</code>: Use with caution. Deletes the contents of <code>FUNC_TEST_PATH</code>.</li> </ul>"},{"location":"dev/testing/#run-functional-testing","title":"Run Functional Testing","text":"<p>To run the default <code>application</code> (based on a standalone use case) test in the default <code>FUNC_TEST_PATH</code>:</p> <pre><code>make func-test\n</code></pre> <p>This will generate the codebase for an operator that uses a standalone workload in your local <code>/tmp/operator-builder-func-test</code> directory.</p> <p>To run the <code>platform</code> test in a non-default directory:</p> <pre><code>FUNC_TEST_PATH=/path/to/platform-operator \\\n  TEST_WORKLOAD_PATH=test/platform \\\n  make func-test\n</code></pre> <p>This will generate the cdoebase for an operator that uses a workload collection in the <code>/path/to/platform-operator</code> directory.</p>"},{"location":"dev/testing/#new-functional-tests","title":"New Functional Tests","text":"<p>Follow these steps to create a new test case:</p> <ol> <li>Create a descriptive name for the test by creating a new directory under    the <code>test/</code> directory.</li> <li>Create a <code>.workloadConfig</code> directory within your newly created directory.</li> <li>Add the YAML files for your workload under the newly created <code>.workloadConfig</code>    directory.</li> <li>Create a workload configuration in the <code>.workloadConfig</code> directory    with the name <code>workload.yaml</code>.</li> </ol>"},{"location":"dev/testing/#run-e2e-testing","title":"Run E2E Testing","text":"<p>As part of the generated code repo, operator-builder will lay down a set of E2E tests that are meant to be run either during normal operating conditions, to check status of the operator, or while make changes to the operator, perhaps as part of a CI/CD or GitOps workflow.</p> <p>There are a few different scenarios to consider when running E2E tests:</p>"},{"location":"dev/testing/#scenario-1-default-crds-installed-in-cluster-controller-running","title":"Scenario 1 (Default): CRDs Installed in Cluster, Controller Running","text":"<p>This is the default scenario as it is generally the quickest and easiest to spin up.  It is also the least invasive and will cause the least amount of headache in the instance that the <code>make test-e2e</code> target is run, accidentally, against an operational cluster.  In this scenario, the E2E test assumes that the custom CRDs are installed into the cluster and the controller is either deployed in the cluster, or running with the <code>make run</code> target.</p> <p>To test this scenario, simply run:</p> <pre><code>make test-e2e\n</code></pre>"},{"location":"dev/testing/#scenario-2-crds-not-installed-in-cluster-controller-running","title":"Scenario 2: CRDs Not Installed in Cluster, Controller Running","text":"<p>This scenario allows you to deploy the CRDs into the cluster as part of E2E testing, but also makes the assumption that the the controller is either deployed in the cluster, or running with the <code>make run</code> target.</p> <p>To test this scenario, simply run:</p> <pre><code>DEPLOY=\"true\" make test-e2e\n</code></pre>"},{"location":"dev/testing/#scenario-3-crds-not-installed-in-cluster-controller-not-running","title":"Scenario 3: CRDs Not Installed in Cluster, Controller Not Running","text":"<p>This scenario allows you to deploy the CRDs into the cluster as part of E2E testing and additionally run through the workflow to deploy the controller into the cluster.  This is the most robust of all of the tests and should be considered for CI/CD or GitOps workflows for testing.</p> <p>To test this scenario, simply run:</p> <pre><code>DEPLOY=\"true\" DEPLOY_IN_CLUSTER=\"true\" IMG=\"my-repo/my-image:my-tag\" make test-e2e\n</code></pre>"},{"location":"dev/testing/#additional-options","title":"Additional Options","text":"<p>Additionally, the following environment variables are available as options for the E2E Testing:</p> <ul> <li><code>TEARDOWN</code>: when set to \"true\", the teardown procedures are run and tested.  You   may not want to set this if you need to inspect the cluster in the case of failed   deployments.</li> </ul>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/","title":"Workload Collection Tutorial","text":"<p>This tutorial walks through all the steps to create an operator that manages multiple distinct workloads using Operator Builder.</p> <p>The operator that you'll build in this tutorial installs supporting services for a cluster.  Specifically, it will install Cert Manager and the Nginx Ingress Controller.</p> <p>Note: all of the manifests and configurations in these instructions can be found in the accompanying <code>.operator-builder</code> directory.</p>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#prerequisites","title":"Prerequisites","text":"<ul> <li>git</li> <li>golang</li> <li>operator-builder</li> </ul>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#setup","title":"Setup","text":"<p>In this section we will create a new project and add the workload configurations for operator-builder.</p> <p>Make a new directory for the project and initialize git.</p> <pre><code>mkdir supporting-services-operator\ncd supporting-services-operator\ngit init\n</code></pre> <p>Create a config directory and add the WorkloadCollection config.</p> <pre><code>mkdir .operator-builder\ncd .operator-builder\noperator-builder init-config collection &gt; workload.yaml\n</code></pre> <p>Edit the sample config so that it looks as follows.</p> <pre><code>kind: WorkloadCollection\nname: supporting-services-collection\nspec:\n  api:\n    clusterScoped: true\n    domain: nukleros.io\n    group: addons\n    kind: SupportingServices\n    version: v1alpha1\n  companionCliRootcmd:\n    description: \"Manage a cluster's supporting service installations\"\n    name: ssctl\n  companionCliSubcmd:\n    description: Manage the collection of services\n    name: collection\n  componentFiles:\n    - addons.nukleros.io_tls/workload.yaml\n    - addons.nukleros.io_ingress/workload.yaml\n  resources: []\n</code></pre> <p>Add the ComponentWorkload configs:</p> <pre><code>mkdir addons.nukleros.io_tls\noperator-builder init-config component &gt; addons.nukleros.io_tls/workload.yaml\nmkdir addons.nukleros.io_ingress\noperator-builder init-config component &gt; addons.nukleros.io_ingress/workload.yaml\n</code></pre> <p>Edit the TLS component workload config so that it looks as follows.</p> <pre><code>kind: ComponentWorkload\nname: tls-component\nspec:\n  api:\n    clusterScoped: true\n    domain: nukleros.io\n    group: addons\n    kind: TLSComponent\n    version: v1alpha1\n  companionCliSubcmd:\n    description: Manage the TLS management service component\n    name: tls\n  resources: []\n</code></pre> <p>Edit the ingress component workload config so that it looks as follows.  Note that we have specified the name of the tls-component workload in the <code>dependencies</code> section.  This indicates that the TLS component must be deployed and ready before the Ingress component can be installed.  In this case, this is because the Ingress component uses a Certificate resource which will not be recognized by the cluster until the relevant CRD is created during installation of the TLS component.</p> <pre><code>kind: ComponentWorkload\nname: ingress-component\nspec:\n  api:\n    clusterScoped: true\n    domain: nukleros.io\n    group: addons\n    kind: IngressComponent\n    version: v1alpha1\n  companionCliSubcmd:\n    description: Manage the ingress service component\n    name: ingress\n  dependencies:\n    - tls-component\n  resources: []\n</code></pre>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#resource-manifests","title":"Resource Manifests","text":"<p>In this section we'll download the resource manifests.  These are Kubernetes manifests for the resources that we want our operator to manage.  They will contain operator-builder markers to indicate which fields need to be configurable through custom resources.</p> <p>Clone the operator-builder repo so as to access the manifests for this tutorial.</p> <pre><code>git clone git@github.com:nukleros/operator-builder.git /tmp/operator-builder\n</code></pre>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#supporting-services-workload-collection","title":"Supporting Services Workload Collection","text":"<p>The only resource associated with the workload collection is a namespace.  No other resources will be created in this namespace at this time.  The namespace resource manifest will be used to create some fields for the SupportingServices custom resource - these will be global configurations that can be used by multiple component workloads.  And the values configured will be added to the namespace as labels for easy reference to what those global configs are.</p> <p>Copy the collection resource manifest for this tutorial.</p> <pre><code>cp /tmp/operator-builder/docs/workload-collection-tutorial/.operator-builder/namespace.yaml .\n</code></pre>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-buildernamespaceyaml","title":"<code>.operator-builder/namespace.yaml</code>","text":"<p>The marker on line 4 makes the name of the namespace name configurable with the name of the SupportServices custom resource.</p> <p>The marker on line 6 creates a new field for the SupportingServices custom resource called <code>cloudProvider</code>.  The Ingress Component will use this field to determine the configuration for that component's Service resource. Normally, we would use global configs for attributes that are shared by multiple components.  However, in this case only the Ingress component uses it.  That is because there is a reasonable expectation that other components will use it in future.  This marker uses the description field to add a kubebuilder marker on the following line that is used to inform the valid values for this field.  You can learn more about kubebuilders validation markers in the kubebuilder docs.</p> <p>The marker on line 9 adds a <code>certProvider</code> field to the SupportingServices custom resource.  It also includes a kubebuilder validation marker to inform the valid values.  This field will configure the issuer name on the ingress default servier Certificate resource as well as on the ClusterIssuer resources created with the TLS component.</p> <p>Update the WorkloadCollection config to include this file under <code>resources</code>.  It should look as follows.</p> <pre><code>kind: WorkloadCollection\nname: supporting-services-collection\nspec:\n  api:\n    clusterScoped: true\n    domain: nukleros.io\n    group: addons\n    kind: SupportingServices\n    version: v1alpha1\n  companionCliRootcmd:\n    description: \"Manage a cluster's supporting service installations\"\n    name: ssctl\n  companionCliSubcmd:\n    description: Manage the collection of services\n    name: collection\n  componentFiles:\n    - addons.nukleros.io_tls/workload.yaml\n    - addons.nukleros.io_ingress/workload.yaml\n  resources:\n    - namespace.yaml\n</code></pre>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#tls-component-workload","title":"TLS Component Workload","text":"<p>The resources for this component will deploy Cert Manager for TLS asset management.</p> <p>Copy the resource manifests for the TLS component for this tutorial.</p> <pre><code>cp -R /tmp/operator-builder/docs/workload-collection-tutorial/.operator-builder/addons.nukleros.io_tls .\n</code></pre>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-builderaddonsnuklerosio_tlsconfigyaml","title":"<code>.operator-builder/addons.nukleros.io_tls/config.yaml</code>","text":"<p>This manifest has just a single marker on line 6.  This marker uses the namespace field from the TLSComponent custom resource to set the namespace for this ConfigMap.  The same will be done for all namespaced resources for this component.  This field has a default value included which makes it an optional field in the TLSComponent resource.</p>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-builderaddonsnuklerosio_tlscrdyaml","title":"<code>.operator-builder/addons.nukleros.io_tls/crd.yaml</code>","text":"<p>This manifest has no operator-builder markers.  The CustomResourceDefinitions are not configurable in any way.</p>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-builderaddonsnuklerosio_tlsdeploymentyaml","title":"<code>.operator-builder/addons.nukleros.io_tls/deployment.yaml</code>","text":"<p>This manifest contains 3 deployment manifests.  The namespace for each is defined in the same way as the ConfigMap.</p> <p>This manifest also includes markers to set versions and replica counts for each Deployment.  For example, on line 12 a field is defined with <code>field:name=caInjector.version</code> that uses dot notation to indicate a nested field.  This field will be used to specify the version for the CA Injector image.  Here it sets a label value, as it does on line 29.  On line 37 the same field is used to add the version to the Deployment's image.  In this case <code>replace=\"caInjectorVersion\"</code> indicates the matching string in the image field's value should be replaced with the value from the TLSComponents custom resource.</p> <p>The number of replicas is similarly configured with a <code>caInjector.replicas</code> field from the TLSComponent resource with the marker on line 15.</p> <p>Separate corresponding markers are set on the other two Deployments in the same file.</p>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-builderaddonsnuklerosio_tlsissuersyaml","title":"<code>.operator-builder/addons.nukleros.io_tls/issuers.yaml</code>","text":"<p>This file contains two ClusterIssuer resources.  The markers on lines 2 and 29 indicate which of the two resources will be used depending up on the value given in the <code>certProvider</code> field of the collection SupportServices custom resource. The <code>value=\"letsencrypt-staging\",include</code> maker fields indicate that when the value of <code>certProvider</code> matches this value, include this resource.  Otherwise it is ommitted.  Therefore if the SupportingService resource includes <code>certProvider: letsencrypt-staging</code> the first resource will be used (see the the value of the <code>server</code> field on line 13.  If <code>certProvider: letsencrypt-production</code> is in the SupportingServices resource, the second ClusterIssuer resource will be used.  The value from this field is also used to populate the value for the <code>cert-provider</code> as indicated by the marker on line 8.</p> <p>Finally, the marker on line 15 allows the contact email for the cert provider with a required <code>contactEmail</code> field.</p>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-builderaddonsnuklerosio_tlsrbacyaml","title":"<code>.operator-builder/addons.nukleros.io_tls/rbac.yaml</code>","text":"<p>The Namespace resource has its name field set by the name field in the spec of the TLSComponent resource.  This field also informs the namespace field in all other fields for the RBAC resources as required.</p>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-builderaddonsnuklerosio_tlsserviceyaml","title":"<code>.operator-builder/addons.nukleros.io_tls/service.yaml</code>","text":"<p>The namespace fields for the two Service resources are set as before.</p>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-builderaddonsnuklerosio_tlswebhookyaml","title":"<code>.operator-builder/addons.nukleros.io_tls/webhook.yaml</code>","text":"<p>Namespaces are again set using the same field.  However in this case we have to use the <code>replace</code> marker field to set a part of a value, e.g. line 13.</p> <p>Finally, update the tls-component ComponentWorkload config to include the filenames of the manifests we added under <code>resources</code>.</p> <pre><code>kind: ComponentWorkload\nname: tls-component\nspec:\n  api:\n    clusterScoped: true\n    domain: nukleros.io\n    group: addons\n    kind: TLSComponent\n    version: v1alpha1\n  companionCliSubcmd:\n    description: Manage the TLS management service component\n    name: tls\n  resources:\n    - config.yaml\n    - crd.yaml\n    - deployment.yaml\n    - issuers.yaml\n    - rbac.yaml\n    - service.yaml\n    - webhook.yaml\n</code></pre>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#ingress-component-workload","title":"Ingress Component Workload","text":"<p>The resources for this component will install the Nginx Ingress Controller so that tenant workloads can use Ingress resources to expose it to traffic from outside the cluster.</p> <p>Copy the resource manifests for the Ingress component for this tutorial.</p> <pre><code>cp -R /tmp/operator-builder/docs/workload-collection-tutorial/.operator-builder/addons.nukleros.io_ingress .\n</code></pre>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-builderaddonsnuklerosio_ingresscertyaml","title":"<code>.operator-builder/addons.nukleros.io_ingress/cert.yaml</code>","text":"<p>The namespace name on the Certificate resource is set using a namespace field as before, this time using the IngressComponent custom resource.</p> <p>The DNS name is set using a new field <code>domainName</code> that is created for the IngressComponent resource.</p> <p>The <code>certProvider</code> field from the SupportingServices resource is again used here on the Certificate.</p>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-builderaddonsnuklerosio_ingressclassyaml","title":"<code>.operator-builder/addons.nukleros.io_ingress/class.yaml</code>","text":"<p>The IngressClass resource has no markers and so is not configurable in any way.</p>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-builderaddonsnuklerosio_ingressconfigyaml","title":"<code>.operator-builder/addons.nukleros.io_ingress/config.yaml</code>","text":"<p>The ConfigMap's namespace is configured as the others are.</p>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-builderaddonsnuklerosio_ingresscrdyaml","title":"<code>.operator-builder/addons.nukleros.io_ingress/crd.yaml</code>","text":"<p>As with the TLSComponent, the CustomResourceDefinitions have no markers and are not configurable.</p>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-builderaddonsnuklerosio_ingressdeploymentyaml","title":"<code>.operator-builder/addons.nukleros.io_ingress/deployment.yaml</code>","text":"<p>The Deployment's namespace is configured as the others are.</p> <p>The replicas and container image are configured with markers on lines 7 and 23 in the same way the cert-manager Deployments were configured.  These markers create new fields on the IngressComponent custom resource.</p>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-builderaddonsnuklerosio_ingressrbacyaml","title":"<code>.operator-builder/addons.nukleros.io_ingress/rbac.yaml</code>","text":"<p>The Namespace and RBAC resources for the Ingress component have their namespaces configured the same way the TLS component did.</p>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-builderaddonsnuklerosio_ingressserviceyaml","title":"<code>.operator-builder/addons.nukleros.io_ingress/service.yaml</code>","text":"<p>There are four Services defined but only one is used in any given case.  The marker at the beginning of each Service manifest determines which one is used based on the SupportingService's <code>cloudProvider</code> field.</p> <p>Again the namespace for the Service is defined as with the others.</p> <p>Finally, update the ingress-component ComponentWorkload config to include the filenames of the manifests we added under <code>resources</code>.</p> <pre><code>kind: ComponentWorkload\nname: ingress-component\nspec:\n  api:\n    clusterScoped: true\n    domain: nukleros.io\n    group: addons\n    kind: IngressComponent\n    version: v1alpha1\n  companionCliSubcmd:\n    description: Manage the ingress service component\n    name: ingress\n  dependencies:\n    - ../addons.nukleros.io_tls/workload.yaml\n  resources:\n    - cert.yaml\n    - class.yaml\n    - config.yaml\n    - crd.yaml\n    - deployment.yaml\n    - rbac.yaml\n    - service.yaml\n</code></pre>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#code-generation","title":"Code Generation","text":"<p>You'll often find you want re-generate the codebase when you find mistakes or adjustments you want to make in the project.  For this reason it's helpful to have a Makefile in your <code>.operator-builder</code> directory.</p> <p>Copy the Makefile for this tutorial.</p> <pre><code>cp /tmp/operator-builder/docs/workload-collection-tutorial/.operator-builder/Makefile .\n</code></pre> <p>Now it's time to generate the code.</p> <pre><code>make operator-init\nmake operator-create\n</code></pre> <p>That's it.</p>"},{"location":"workload-collection-tutorial/workload-collection-tutorial/#operator-testing","title":"Operator Testing","text":"<p>Now it's time to test your operator.  For this you'll need a working Kubernetes cluster.</p> <p>Navigate up into the root of your operator project's codebase.</p> <pre><code>cd ..\n</code></pre> <p>Your new operator has a Makefile of it's own that has some handy make targets.</p> <p>Install the CRDs into your cluster.</p> <pre><code>make install\n</code></pre> <p>Run the controller locally against your test cluster.</p> <pre><code>make run\n</code></pre> <p>In another terminal, install the custom resource samples.</p> <pre><code>kubectl apply -f config/samples\n</code></pre> <p>If everything has gone according to plan, your cluster will soon have Cert Manager and the Nginx Ingress Controller installed.</p> <p>Congratulations!  You just built a multi-workload operator.</p> <p>Note: The Nginx Ingress Controller will require a valid DNS record that points to your ingress public IP so as to validate your ownership of the domain and for Let's Encrypt to issue a publicly trustable certificate.  Without this, everything will spin up but the Nginx instance will fail to start due to the inability to mount the secret with it's default server certificate.</p>"}]}